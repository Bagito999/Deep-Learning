{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMVD3oyfTCN+fvmmV/tNxr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bagito999/Deep-Learning/blob/main/Chapter_16_Teori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Chapter 16 - Natural Language Processing with RNNs and Attention\n",
        "\n",
        "##  Tujuan Bab\n",
        "\n",
        "Bab ini menjelaskan bagaimana membangun sistem pemrosesan bahasa alami (NLP) menggunakan:\n",
        "- Recurrent Neural Networks (RNN)\n",
        "- LSTM & GRU\n",
        "- Encoder‚ÄìDecoder architecture\n",
        "- Attention mechanism\n",
        "\n",
        "---\n",
        "\n",
        "##  NLP dan Reprepresentasi Teks\n",
        "\n",
        "Teks tidak bisa langsung digunakan sebagai input model; perlu dikonversi menjadi numerik:\n",
        "\n",
        "### üîπ Tokenization\n",
        "- Memecah teks menjadi token (kata, sub-kata, karakter)\n",
        "\n",
        "### üîπ Vectorization\n",
        "- **One-hot encoding**: vektor sparse, besar\n",
        "- **Word embeddings**: dense representation seperti:\n",
        "  - Word2Vec\n",
        "  - GloVe\n",
        "  - FastText\n",
        "\n",
        "### üîπ Embedding Layer\n",
        "\\[\n",
        "$E: \\text{Vocab} \\rightarrow \\mathbb{R}^d$\n",
        "\\]\n",
        "Mempelajari representasi kata berdimensi tetap selama training.\n",
        "\n",
        "---\n",
        "\n",
        "##  Arsitektur Encoder‚ÄìDecoder\n",
        "\n",
        "Digunakan untuk:\n",
        "- Machine Translation\n",
        "- Summarization\n",
        "- Text Generation\n",
        "\n",
        "### üî∏ Encoder:\n",
        "- Mengubah input sequence menjadi vektor konteks tetap:\n",
        "\\[\n",
        "$\\mathbf{c} = h_T$\n",
        "\\]\n",
        "\n",
        "### üî∏ Decoder:\n",
        "- Menghasilkan output sequence dari konteks:\n",
        "\\[\n",
        "$s_t = f(s_{t-1}, y_{t-1}, \\mathbf{c})$\n",
        "\\]\n",
        "\n",
        "Masalah:\n",
        "- Konteks tetap (fixed-size vector) membuat sulit untuk menyimpan seluruh informasi ‚Üí Solusi: **Attention**\n",
        "\n",
        "---\n",
        "\n",
        "##  Mekanisme Attention\n",
        "\n",
        "Membuat model fokus pada bagian penting input saat menghasilkan setiap token output.\n",
        "\n",
        "### üîπ Skema Umum:\n",
        "\n",
        "1. Untuk setiap timestep decoder \\($ t $\\), hitung perhatian terhadap encoder hidden states \\($ h_1, ..., h_T $\\)\n",
        "2. Hitung skor:\n",
        "\n",
        "\\[\n",
        "$score(s_t, h_i) = s_t^T W_a h_i$\n",
        "\\]\n",
        "\n",
        "3. Gunakan Softmax:\n",
        "\n",
        "\\[\n",
        "$\\alpha_{t,i} = \\frac{\\exp(score(s_t, h_i))}{\\sum_j \\exp(score(s_t, h_j))}$\n",
        "\\]\n",
        "\n",
        "4. Hitung konteks dinamis:\n",
        "\n",
        "\\[\n",
        "$c_t = \\sum_i \\alpha_{t,i} h_i$\n",
        "\\]\n",
        "\n",
        "5. Decoder menghasilkan output berdasarkan \\($ s_{t-1}, y_{t-1}, c_t $\\)\n",
        "\n",
        "---\n",
        "\n",
        "##  Jenis Attention\n",
        "\n",
        "### üîπ Bahdanau (Additive Attention)\n",
        "\n",
        "\\[\n",
        "$score(s_t, h_i) = v_a^T \\tanh(W_s s_t + W_h h_i)$\n",
        "\\]\n",
        "\n",
        "### üîπ Luong (Multiplicative / Dot Product)\n",
        "\n",
        "\\[\n",
        "$score(s_t, h_i) = s_t^T W h_i$\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "##  Aplikasi NLP dengan Attention\n",
        "\n",
        "1. **Machine Translation**\n",
        "   - Input: kalimat dalam bahasa sumber\n",
        "   - Output: kalimat dalam bahasa target\n",
        "   - Attention penting untuk menangani kalimat panjang\n",
        "\n",
        "2. **Text Summarization**\n",
        "   - Input: dokumen\n",
        "   - Output: ringkasan singkat\n",
        "\n",
        "3. **Text Generation**\n",
        "   - Gunakan RNN + embedding + attention\n",
        "\n",
        "4. **Question Answering**\n",
        "   - Model fokus pada bagian dokumen yang menjawab pertanyaan\n",
        "\n",
        "---\n",
        "\n",
        "##  Word Embedding Pretrained\n",
        "\n",
        "Gunakan model embedding terlatih seperti:\n",
        "- GloVe\n",
        "- FastText\n",
        "- BERT embeddings (contextual)\n",
        "\n",
        "Keuntungan:\n",
        "- Mengurangi kebutuhan data besar\n",
        "- Mempercepat konvergensi\n",
        "\n",
        "---\n",
        "\n",
        "##  Contextual Embeddings\n",
        "\n",
        "Model modern seperti:\n",
        "- **ELMo**\n",
        "- **BERT**\n",
        "- **GPT**\n",
        "\n",
        "Menghasilkan embedding bergantung konteks: ‚Äúbank‚Äù pada ‚Äúriver bank‚Äù ‚â† ‚Äúfinancial bank‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "##  Teacher Forcing\n",
        "\n",
        "Saat melatih decoder, gunakan token target sebagai input alih-alih prediksi sebelumnya.\n",
        "\n",
        "- Mempercepat pelatihan\n",
        "- Tapi bisa menimbulkan ketergantungan pada data ground-truth saat inference (exposure bias)\n",
        "\n",
        "---\n",
        "\n",
        "##  Beam Search (Inference)\n",
        "\n",
        "Selama prediksi sequence:\n",
        "- Alih-alih pilih token terbaik (greedy), simpan top-k kandidat (beam)\n",
        "- Menjaga keseimbangan antara akurasi dan efisiensi\n",
        "\n",
        "---\n",
        "\n",
        "##  Evaluasi NLP\n",
        "\n",
        "1. **BLEU Score**: metrik untuk translation\n",
        "2. **ROUGE**: untuk summarization\n",
        "3. **Perplexity**: untuk language modeling\n",
        "4. **Accuracy**: untuk klasifikasi teks\n",
        "\n",
        "---\n",
        "\n",
        "##  Kesimpulan\n",
        "\n",
        "- NLP membutuhkan representasi teks numerik ‚Üí embedding\n",
        "- Encoder‚ÄìDecoder cocok untuk sequence-to-sequence task\n",
        "- Attention memungkinkan model memfokuskan konteks secara dinamis\n",
        "- Pretrained embeddings dan contextual models meningkatkan hasil\n",
        "- Beam search dan teacher forcing mengoptimalkan prediksi\n",
        "\n",
        "---\n",
        "\n",
        "##  Referensi\n",
        "\n",
        "G√©ron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*. O'Reilly Media.\n"
      ],
      "metadata": {
        "id": "i7_e8wA8TeQ-"
      }
    }
  ]
}