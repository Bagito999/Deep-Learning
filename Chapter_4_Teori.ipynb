{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6PDN/4Ir6FOYevGDIC/3L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bagito999/Deep-Learning/blob/main/Chapter_4_Teori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Chapter 4 - Training Models\n",
        "\n",
        "##  Pendahuluan\n",
        "\n",
        "Bab ini membahas **apa yang terjadi di balik layar saat melatih model Machine Learning**, khususnya:\n",
        "- Linear Regression\n",
        "- Gradient Descent\n",
        "- Regularisasi\n",
        "- Logistic Regression\n",
        "- Softmax Regression\n",
        "\n",
        "---\n",
        "\n",
        "##  Linear Regression\n",
        "\n",
        "Model Linear Regression memprediksi target sebagai kombinasi linear dari fitur:\n",
        "\n",
        "### ðŸ”¹ Rumus Linear Regression\n",
        "\n",
        "\\[\n",
        "$\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\dots + \\theta_nx_n$\n",
        "\\]\n",
        "\n",
        "atau dalam bentuk vektor:\n",
        "\n",
        "\\[\n",
        "$\\hat{y} = \\mathbf{\\theta}^T \\cdot \\mathbf{x}$\n",
        "\\]\n",
        "\n",
        "### ðŸ”¸ MSE (Mean Squared Error)\n",
        "\n",
        "Fungsi kerugian yang digunakan untuk regresi linier adalah:\n",
        "\n",
        "\\[\n",
        "$\\text{MSE}(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left( \\theta^T x^{(i)} - y^{(i)} \\right)^2$\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "##  Normal Equation\n",
        "\n",
        "Untuk solusi eksak parameter:\n",
        "\n",
        "\\[\n",
        "$\\theta = (X^T X)^{-1} X^T y$\n",
        "\\]\n",
        "\n",
        "Namun, metode ini **tidak efisien** jika fitur terlalu banyak (kompleksitas waktu tinggi).\n",
        "\n",
        "---\n",
        "\n",
        "##  Gradient Descent\n",
        "\n",
        "###  Prinsip\n",
        "Gradient Descent adalah metode iteratif untuk meminimalkan cost function.\n",
        "\n",
        "### ðŸ”¹ Update rule:\n",
        "\n",
        "\\[\n",
        "$\\theta := \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta)$\n",
        "\\]\n",
        "\n",
        "- \\($ \\eta $\\): learning rate\n",
        "- \\($ \\nabla_\\theta J(\\theta) $\\): turunan dari fungsi kerugian terhadap parameter\n",
        "\n",
        "### Jenis:\n",
        "1. **Batch Gradient Descent** â€“ menggunakan seluruh dataset\n",
        "2. **Stochastic Gradient Descent (SGD)** â€“ satu instance per iterasi\n",
        "3. **Mini-batch Gradient Descent** â€“ subset kecil (batch)\n",
        "\n",
        "---\n",
        "\n",
        "##  Polynomial Regression\n",
        "\n",
        "Model yang memproyeksikan data ke fitur polinomial agar bisa mempelajari hubungan nonlinier.\n",
        "\n",
        "###  Tantangan:\n",
        "- **Overfitting** jika derajat terlalu tinggi\n",
        "- **Underfitting** jika derajat terlalu rendah\n",
        "\n",
        "---\n",
        "\n",
        "##  Learning Curves\n",
        "\n",
        "Visualisasi error terhadap ukuran data pelatihan, menunjukkan apakah model overfit atau underfit:\n",
        "\n",
        "- **Training error turun**\n",
        "- **Validation error naik** â†’ tanda overfitting\n",
        "\n",
        "---\n",
        "\n",
        "##  Regularisasi\n",
        "\n",
        "Untuk mencegah overfitting dengan menambahkan penalti terhadap bobot parameter.\n",
        "\n",
        "### ðŸ”¸ Ridge Regression (L2)\n",
        "\n",
        "\\[\n",
        "$J(\\theta) = \\text{MSE}(\\theta) + \\alpha \\sum_{j=1}^{n} \\theta_j^2$\n",
        "\\]\n",
        "\n",
        "### ðŸ”¸ Lasso Regression (L1)\n",
        "\n",
        "\\[\n",
        "$J(\\theta) = \\text{MSE}(\\theta) + \\alpha \\sum_{j=1}^{n} |\\theta_j|$\n",
        "\\]\n",
        "\n",
        "### ðŸ”¸ Elastic Net\n",
        "\n",
        "Kombinasi L1 dan L2:\n",
        "\n",
        "\\[\n",
        "$J(\\theta) = \\text{MSE}(\\theta) + r \\cdot \\sum |\\theta_j| + (1 - r) \\cdot \\sum \\theta_j^2$\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "##  Early Stopping\n",
        "\n",
        "Berhenti melatih model saat error pada validation set mulai naik, mencegah overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "##  Logistic Regression\n",
        "\n",
        "Digunakan untuk klasifikasi biner.\n",
        "\n",
        "### ðŸ”¹ Hipotesis (Fungsi Sigmoid)\n",
        "\n",
        "\\[\n",
        "$\\hat{p} = \\sigma(z) = \\frac{1}{1 + e^{-z}} \\quad \\text{dengan } z = \\theta^T x$\n",
        "\\]\n",
        "\n",
        "### ðŸ”¸ Cost Function (Log Loss)\n",
        "\n",
        "\\[\n",
        "$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(\\hat{p}^{(i)}) + (1 - y^{(i)}) \\log(1 - \\hat{p}^{(i)}) \\right]$\n",
        "\\]\n",
        "\n",
        "Turunan parsialnya:\n",
        "\n",
        "\\[\n",
        "$\\frac{\\partial J}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{p}^{(i)} - y^{(i)} \\right) x_j^{(i)}$\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "##  Softmax Regression (Multiclass)\n",
        "\n",
        "Generalisasi Logistic Regression untuk klasifikasi multi-kelas.\n",
        "\n",
        "### ðŸ”¸ Skor untuk setiap kelas \\( k \\):\n",
        "\n",
        "\\[\n",
        "$s_k(x) = x^T \\theta^{(k)}$\n",
        "\\]\n",
        "\n",
        "### ðŸ”¹ Softmax function:\n",
        "\n",
        "\\[\n",
        "$\\hat{p}_k = \\frac{e^{s_k(x)}}{\\sum_{j=1}^{K} e^{s_j(x)}}$\n",
        "\\]\n",
        "\n",
        "### ðŸ”¸ Cost Function (Cross Entropy):\n",
        "\n",
        "\\[\n",
        "$J(\\Theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log(\\hat{p}_k^{(i)})$\n",
        "\\]\n",
        "\n",
        "### ðŸ”¹ Gradien:\n",
        "\n",
        "\\[\n",
        "$\\nabla_{\\theta^{(k)}} J(\\Theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\hat{p}_k^{(i)} - y_k^{(i)} \\right) x^{(i)}$\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "##  Regularisasi Logistic/Softmax Regression\n",
        "\n",
        "LogisticRegression di Scikit-Learn menggunakan regularisasi L2 secara default.\n",
        "\n",
        "- **Hyperparameter:** `C` = kebalikan dari kekuatan regularisasi\n",
        "  - Nilai besar â†’ regularisasi lemah\n",
        "  - Nilai kecil â†’ regularisasi kuat\n",
        "\n",
        "---\n",
        "\n",
        "##  Kesimpulan\n",
        "\n",
        "- Linear Regression dapat dilatih dengan **Normal Equation** atau **Gradient Descent**.\n",
        "- **Gradient Descent** memiliki varian: batch, mini-batch, dan stochastic.\n",
        "- **Regularisasi** sangat penting untuk menghindari overfitting.\n",
        "- Logistic dan Softmax Regression digunakan untuk klasifikasi.\n",
        "- Pemahaman terhadap rumus, gradien, dan cost function sangat penting untuk membangun sistem ML yang efisien dan handal.\n",
        "\n",
        "---\n",
        "\n",
        "##  Referensi\n",
        "\n",
        "GÃ©ron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*. O'Reilly Media.\n"
      ],
      "metadata": {
        "id": "9LwqeL4ba1Si"
      }
    }
  ]
}