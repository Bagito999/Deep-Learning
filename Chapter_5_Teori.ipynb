{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTN+Jumssr+w01brOmclMm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bagito999/Deep-Learning/blob/main/Chapter_5_Teori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Chapter 5 - Support Vector Machines\n",
        "\n",
        "##  Pendahuluan\n",
        "\n",
        "**Support Vector Machines (SVMs)** adalah model Machine Learning yang kuat dan fleksibel, mampu menangani:\n",
        "- Klasifikasi linear dan non-linear\n",
        "- Regresi\n",
        "- Deteksi outlier\n",
        "\n",
        "SVM sangat cocok untuk dataset kecil hingga menengah dengan pola yang kompleks.\n",
        "\n",
        "---\n",
        "\n",
        "##  Linear SVM Classification\n",
        "\n",
        "Tujuan utama dari SVM adalah menemukan **hyperplane** (garis keputusan) yang **memisahkan kelas dengan margin selebar mungkin**.\n",
        "\n",
        "### üîπ Konsep \"Large Margin Classification\"\n",
        "SVM mencari batas keputusan yang:\n",
        "- Memisahkan dua kelas\n",
        "- **Jarak maksimum** terhadap data terdekat (support vectors)\n",
        "\n",
        "---\n",
        "\n",
        "##  Support Vectors\n",
        "\n",
        "**Support Vectors** adalah titik data yang berada pada batas margin. Hanya titik-titik ini yang mempengaruhi posisi hyperplane.\n",
        "\n",
        "Menambahkan data di luar margin tidak memengaruhi model.\n",
        "\n",
        "---\n",
        "\n",
        "##  Sensitivitas terhadap Skala Fitur\n",
        "\n",
        "SVM sensitif terhadap skala karena algoritma mencoba memaksimalkan margin. **Feature scaling (standarisasi)** sangat penting.\n",
        "\n",
        "---\n",
        "\n",
        "##  Hard vs Soft Margin Classification\n",
        "\n",
        "### üî∏ Hard Margin\n",
        "- Tidak mengizinkan kesalahan klasifikasi\n",
        "- Hanya bisa digunakan jika data **sepenuhnya terpisah secara linear**\n",
        "- Sangat **sensitif terhadap outlier**\n",
        "\n",
        "### üî∏ Soft Margin\n",
        "- Mengizinkan pelanggaran margin secara terbatas\n",
        "- Tujuan: **keseimbangan antara lebar margin dan error**\n",
        "\n",
        "###  Fungsi Objektif Soft Margin\n",
        "\n",
        "\\[\n",
        "$\\min_{w, b, \\zeta} \\left[ \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^m \\zeta_i \\right]$\n",
        "\\]\n",
        "\n",
        "dengan:\n",
        "\n",
        "\\[\n",
        "$t_i(w^\\top x_i + b) \\geq 1 - \\zeta_i, \\quad \\zeta_i \\geq 0$\n",
        "\\]\n",
        "\n",
        "- \\($ C $\\): hyperparameter yang mengatur trade-off\n",
        "\n",
        "---\n",
        "\n",
        "##  Fungsi Kerugian (Loss Function)\n",
        "\n",
        "### üîπ Hinge Loss\n",
        "\n",
        "\\[\n",
        "$L(t) = \\max(0, 1 - t)$\n",
        "\\]\n",
        "\n",
        "- Hinge loss = 0 jika data diklasifikasikan benar dan cukup jauh dari batas\n",
        "- Meningkat linear jika prediksi salah\n",
        "\n",
        "---\n",
        "\n",
        "##  Kernel Trick\n",
        "\n",
        "###  Tujuan:\n",
        "Menyelesaikan masalah non-linear dengan **mentransformasikan fitur ke ruang berdimensi lebih tinggi**, lalu menggunakan SVM linear.\n",
        "\n",
        "### üîπ Kernel Function\n",
        "\n",
        "Fungsi kernel menghitung:\n",
        "\n",
        "\\[\n",
        "$K(a, b) = \\phi(a)^\\top \\phi(b)$\n",
        "\\]\n",
        "\n",
        "Tanpa perlu menghitung \\($ \\phi(x) $\\) secara eksplisit.\n",
        "\n",
        "### üî∏ Kernel Populer:\n",
        "\n",
        "- **Linear**: \\($ K(a, b) = a^\\top b $\\)\n",
        "- **Polynomial**: \\($ K(a, b) = (\\gamma a^\\top b + r)^d $\\)\n",
        "- **Gaussian RBF**: \\($ K(a, b) = \\exp(-\\gamma \\|a - b\\|^2) $\\)\n",
        "\n",
        "---\n",
        "\n",
        "## üìâ Kompleksitas Komputasi\n",
        "\n",
        "| Kelas          | Kompleksitas Waktu   | Kernel | Scaling Wajib | Out-of-core |\n",
        "|----------------|----------------------|--------|----------------|--------------|\n",
        "| `LinearSVC`    | \\($ O(m \\cdot n) $\\)   | ‚ùå     | ‚úÖ              | ‚ùå           |\n",
        "| `SGDClassifier`| \\($ O(m \\cdot n) $\\)   | ‚ùå     | ‚úÖ              | ‚úÖ           |\n",
        "| `SVC`          | \\($ O(m^2 \\cdot n) $\\) s/d \\($ O(m^3 \\cdot n) $\\) | ‚úÖ | ‚úÖ | ‚ùå |\n",
        "\n",
        "- \\($ m $\\): jumlah instance\n",
        "- \\($ n $\\): jumlah fitur\n",
        "\n",
        "---\n",
        "\n",
        "##  Kernelized SVM dan Dual Problem\n",
        "\n",
        "Untuk kernelisasi, kita gunakan **dual form** dari optimisasi SVM:\n",
        "\n",
        "### üîπ Dual Form Objective\n",
        "\n",
        "\\[\n",
        "$\\min_\\alpha \\left[ \\frac{1}{2} \\sum_{i=1}^m \\sum_{j=1}^m \\alpha_i \\alpha_j t_i t_j K(x_i, x_j) - \\sum_{i=1}^m \\alpha_i \\right]$\n",
        "\\]\n",
        "\n",
        "Dengan \\($ \\alpha_i \\geq 0 $\\)\n",
        "\n",
        "### üî∏ Solusi Primal dari Dual\n",
        "\n",
        "\\[\n",
        "$w = \\sum_{i=1}^{m} \\alpha_i t_i x_i$\n",
        "\\]\n",
        "\\[\n",
        "$b = \\frac{1}{n_s} \\sum_{i \\in SV} \\left( t_i - \\sum_{j=1}^{m} \\alpha_j t_j K(x_j, x_i) \\right)$\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "##  Prediksi dengan Kernelized SVM\n",
        "\n",
        "\\[\n",
        "$f(x) = \\sum_{i \\in SV} \\alpha_i t_i K(x_i, x) + b$\n",
        "\\]\n",
        "\n",
        "Hanya melibatkan **support vectors**, bukan seluruh dataset.\n",
        "\n",
        "---\n",
        "\n",
        "##  SVM untuk Regresi (SVR)\n",
        "\n",
        "### üî∏ Tujuan:\n",
        "Menyesuaikan garis (atau kurva) yang sebanyak mungkin instance-nya berada di dalam margin.\n",
        "\n",
        "### üîπ Fungsi Objektif SVR:\n",
        "\n",
        "\\[\n",
        "$\\min_{w, b} \\left[ \\frac{1}{2} \\|w\\|^2 + C \\sum \\text{pelanggaran margin} \\right]$\n",
        "\\]\n",
        "\n",
        "- Margin tidak boleh dilewati kecuali dibayar dengan penalti\n",
        "- Hyperparameter \\($ \\epsilon $\\) mengatur lebar margin\n",
        "\n",
        "---\n",
        "\n",
        "##  Online SVM\n",
        "\n",
        "Untuk SVM linear, bisa gunakan **SGDClassifier** dengan loss hinge.\n",
        "\n",
        "Fungsi kerugiannya:\n",
        "\n",
        "\\[\n",
        "$J(w, b) = \\frac{1}{2} w^\\top w + C \\sum_{i=1}^{m} \\max(0, 1 - t_i (w^\\top x_i + b))$\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "##  Kesimpulan\n",
        "\n",
        "- SVM mencari hyperplane dengan **margin terluas**\n",
        "- **Support vectors** adalah inti dari pembelajaran SVM\n",
        "- Kernel trick memungkinkan klasifikasi non-linear tanpa eksplisit menambah fitur\n",
        "- Cocok untuk dataset kecil/menengah, kompleks, dan non-linear\n",
        "- SVR memperluas konsep SVM ke tugas regresi\n",
        "\n",
        "---\n",
        "\n",
        "##  Referensi\n",
        "\n",
        "G√©ron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*. O'Reilly Media.\n"
      ],
      "metadata": {
        "id": "qH3NtqOTcl8C"
      }
    }
  ]
}