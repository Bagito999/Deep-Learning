{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4utppt/Uu3965JJvFNLzS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bagito999/Deep-Learning/blob/main/Chapter_7_Teori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Chapter 7 - Ensemble Learning and Random Forests\n",
        "\n",
        "##  Apa Itu Ensemble Learning?\n",
        "\n",
        "Ensemble Learning adalah teknik menggabungkan beberapa model Machine Learning (**predictors**) untuk meningkatkan performa prediksi. Kumpulan model disebut **ensemble**, dan metode penggabungannya disebut **metode ensemble**.\n",
        "\n",
        "Contohnya:\n",
        "- Menggabungkan beberapa pohon keputusan (Decision Trees) menjadi satu model prediktif yang lebih kuat: **Random Forest**.\n",
        "\n",
        "---\n",
        "\n",
        "##  Voting Classifiers\n",
        "\n",
        "**Hard Voting**:\n",
        "- Setiap model memilih kelas.\n",
        "- Kelas yang paling banyak dipilih menjadi prediksi akhir.\n",
        "\n",
        "**Soft Voting**:\n",
        "- Setiap model menghitung probabilitas setiap kelas.\n",
        "- Probabilitas dijumlahkan dan kelas dengan rata-rata tertinggi dipilih.\n",
        "\n",
        " Ensemble bekerja efektif jika model-model penyusunnya **beragam dan independen**.\n",
        "\n",
        "---\n",
        "\n",
        "##  Bagging dan Pasting\n",
        "\n",
        "**Bagging** (*Bootstrap Aggregating*):\n",
        "- Setiap model dilatih dengan subset acak dari data training **dengan pengembalian (replacement)**.\n",
        "\n",
        "**Pasting**:\n",
        "- Sama seperti Bagging, tapi **tanpa pengembalian**.\n",
        "\n",
        "Kedua teknik ini mengurangi **variance** sambil mempertahankan bias rendah.\n",
        "\n",
        "---\n",
        "\n",
        "##  Out-of-Bag (OOB) Evaluation\n",
        "\n",
        "Setiap model dalam ensemble Bagging tidak melihat seluruh data training.\n",
        "- Data yang tidak digunakan (out-of-bag) dapat digunakan untuk mengevaluasi model tanpa perlu validation set tambahan.\n",
        "\n",
        "---\n",
        "\n",
        "##  Random Patches dan Random Subspaces\n",
        "\n",
        "- **Random Patches**: sampling **fitur dan instance**\n",
        "- **Random Subspaces**: sampling **fitur saja**\n",
        "\n",
        "Meningkatkan keragaman model dan mengurangi korelasi antar model.\n",
        "\n",
        "---\n",
        "\n",
        "##  Random Forests\n",
        "\n",
        "**Random Forest** adalah sekumpulan Decision Tree yang dibangun dengan:\n",
        "- Metode **Bagging**\n",
        "- **Fitur acak** yang dipilih untuk setiap split\n",
        "\n",
        "Hal ini meningkatkan keragaman, menurunkan variance, dan menjaga bias tetap rendah.\n",
        "\n",
        "---\n",
        "\n",
        "##  Extra-Trees (Extremely Randomized Trees)\n",
        "\n",
        "- Mirip Random Forest, tapi:\n",
        "  - Gunakan threshold acak (bukan terbaik) untuk split\n",
        "  - Lebih cepat karena tidak mencari split optimal\n",
        "\n",
        "Extra randomness â†’ lebih cepat & mengurangi variance\n",
        "\n",
        "---\n",
        "\n",
        "##  Feature Importance\n",
        "\n",
        "Random Forest dapat mengukur pentingnya fitur dengan:\n",
        "- Melihat seberapa besar impurity berkurang oleh setiap fitur\n",
        "- Semakin besar penurunan impurity â†’ fitur makin penting\n",
        "\n",
        "Hasilnya di-*normalisasi* agar total pentingnya fitur = 1.\n",
        "\n",
        "---\n",
        "\n",
        "##  Boosting\n",
        "\n",
        "Boosting adalah metode ensemble yang **melatih model secara berurutan**, di mana setiap model baru mencoba **memperbaiki kesalahan** model sebelumnya.\n",
        "\n",
        "Dua jenis populer:\n",
        "- **AdaBoost**\n",
        "- **Gradient Boosting**\n",
        "\n",
        "---\n",
        "\n",
        "##  AdaBoost (Adaptive Boosting)\n",
        "\n",
        "Cara kerja:\n",
        "1. Inisialisasi bobot instance sama rata\n",
        "2. Latih model â†’ hitung error tertimbang\n",
        "3. Tingkatkan bobot instance yang salah diklasifikasikan\n",
        "4. Latih model berikutnya\n",
        "5. Gabungkan model dengan voting berbobot\n",
        "\n",
        "### ðŸ”¹ Rumus Error Tertimbang:\n",
        "\n",
        "\\[\n",
        "$r_j = \\frac{\\sum_{i=1}^{m} w_i \\cdot \\mathbf{1}(y_i \\neq h_j(x_i))}{\\sum_{i=1}^{m} w_i}$\n",
        "\\]\n",
        "\n",
        "### ðŸ”¸ Bobot Model:\n",
        "\n",
        "\\[\n",
        "$\\alpha_j = \\eta \\cdot \\log\\left( \\frac{1 - r_j}{r_j} \\right)$\n",
        "\\]\n",
        "\n",
        "### ðŸ”» Update Bobot Instance:\n",
        "\n",
        "\\[\n",
        "$w_i \\leftarrow w_i \\cdot \\exp(\\alpha_j) \\quad \\text{jika salah}$\n",
        "\\]\n",
        "\n",
        "### ðŸ”º Prediksi Final:\n",
        "\n",
        "\\[\n",
        "$\\hat{y}(x) = \\arg\\max_k \\sum_{j=1}^{N} \\alpha_j \\cdot \\mathbf{1}(h_j(x) = k)$\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "##  Gradient Boosting\n",
        "\n",
        "Gradient Boosting membangun model secara berurutan, tetapi alih-alih mengubah bobot data, ia **mencoba mengurangi residual (error)** dari model sebelumnya.\n",
        "\n",
        "### ðŸ”¹ Fungsi Tujuan:\n",
        "Untuk regresi MSE:\n",
        "\n",
        "\\[\n",
        "$\\hat{y}_m(x) = \\hat{y}_{m-1}(x) + \\eta \\cdot h_m(x)$\n",
        "\\]\n",
        "\n",
        "- \\($ h_m(x) $\\): model ke-m yang memprediksi residual\n",
        "- \\($ \\eta $\\): learning rate (pengontrol langkah perbaikan)\n",
        "\n",
        "### ðŸ”º Early Stopping:\n",
        "Berhenti saat error validasi mulai meningkat untuk mencegah overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "##  XGBoost (Extreme Gradient Boosting)\n",
        "\n",
        "- Implementasi Gradient Boosting yang **efisien dan cepat**\n",
        "- Mendukung **early stopping**, **parallel training**, dan **regularisasi**\n",
        "- Sangat populer di kompetisi seperti Kaggle\n",
        "\n",
        "---\n",
        "\n",
        "##  Stacking (Stacked Generalization)\n",
        "\n",
        "Alih-alih voting, **Stacking** menggunakan model tambahan (**blender/meta-learner**) untuk menggabungkan output dari beberapa model dasar.\n",
        "\n",
        "Cara kerja:\n",
        "1. Latih beberapa model dasar pada subset data\n",
        "2. Gunakan prediksi mereka untuk membuat dataset baru\n",
        "3. Latih model meta di atas prediksi model dasar\n",
        "\n",
        "Dapat meningkatkan akurasi lebih baik dari voting biasa.\n",
        "\n",
        "---\n",
        "\n",
        "##  Kesimpulan\n",
        "\n",
        "- Ensemble Learning meningkatkan akurasi model\n",
        "- Bagging mengurangi variance, Boosting mengurangi bias\n",
        "- Random Forests sangat kuat dan fleksibel\n",
        "- Stacking menawarkan metode penggabungan yang lebih canggih\n",
        "- Gunakan cross-validation untuk memilih metode ensemble terbaik\n",
        "\n",
        "---\n",
        "\n",
        "##  Referensi\n",
        "\n",
        "GÃ©ron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*. O'Reilly Media.\n"
      ],
      "metadata": {
        "id": "qz8PU4PhgTDw"
      }
    }
  ]
}