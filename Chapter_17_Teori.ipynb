{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKkJU+AaVeuQYDy76NvxiG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bagito999/Deep-Learning/blob/main/Chapter_17_Teori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Chapter 17 - Transformers and Pretraining\n",
        "\n",
        "##  Tujuan Bab\n",
        "\n",
        "Memahami arsitektur **Transformer** yang menjadi fondasi model NLP modern seperti:\n",
        "- BERT\n",
        "- GPT\n",
        "- T5\n",
        "\n",
        "Bab ini juga membahas konsep **pretraining** dan **fine-tuning** pada model skala besar.\n",
        "\n",
        "---\n",
        "\n",
        "##  Masalah pada RNN & CNN untuk NLP\n",
        "\n",
        "RNN:\n",
        "- Sulit paralelisasi\n",
        "- Rentan terhadap vanishing gradients\n",
        "- Lambat saat mengolah sekuens panjang\n",
        "\n",
        "CNN:\n",
        "- Tidak dapat menangkap dependensi jangka panjang secara efisien\n",
        "\n",
        "Transformer mengatasi masalah tersebut dengan **Self-Attention** dan pemrosesan paralel.\n",
        "\n",
        "---\n",
        "\n",
        "##  Transformer Overview\n",
        "\n",
        "Arsitektur dari Vaswani et al. (2017) ‚Äì ‚ÄúAttention is All You Need‚Äù\n",
        "\n",
        "### üîπ Komponen Utama:\n",
        "\n",
        "1. **Encoder‚ÄìDecoder**:\n",
        "   - Encoder: memproses input\n",
        "   - Decoder: menghasilkan output\n",
        "\n",
        "2. **Self-Attention**:\n",
        "   - Mengizinkan setiap posisi input memperhatikan semua posisi lain\n",
        "\n",
        "3. **Positional Encoding**:\n",
        "   - Menambahkan informasi urutan (karena tidak ada struktur sekuens eksplisit)\n",
        "\n",
        "---\n",
        "\n",
        "##  Scaled Dot-Product Attention\n",
        "\n",
        "### Rumus:\n",
        "\n",
        "\\[\n",
        "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
        "\\]\n",
        "\n",
        "- \\($ Q $\\): Query\n",
        "- \\($ K $\\): Key\n",
        "- \\($ V $\\): Value\n",
        "- \\($ d_k $\\): Dimensi key\n",
        "\n",
        "### Penjelasan:\n",
        "- Hitung kemiripan antara query dan key ‚Üí bobot perhatian\n",
        "- Gunakan bobot untuk kombinasi linier value\n",
        "\n",
        "---\n",
        "\n",
        "##  Multi-Head Attention\n",
        "\n",
        "Alih-alih satu attention, Transformer menggunakan **beberapa ‚Äúhead‚Äù paralel**:\n",
        "\n",
        "\\[\n",
        "$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$\n",
        "\\]\n",
        "\n",
        "- Masing-masing head fokus pada representasi berbeda\n",
        "\n",
        "---\n",
        "\n",
        "##  Positional Encoding\n",
        "\n",
        "Karena Transformer tidak memiliki urutan implisit seperti RNN, digunakan encoding sinusoidal:\n",
        "\n",
        "\\[\n",
        "$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$\n",
        "\\]\n",
        "\\[\n",
        "$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "##  Struktur Encoder\n",
        "\n",
        "Setiap encoder layer:\n",
        "1. **Multi-Head Self-Attention**\n",
        "2. **Add & Norm**\n",
        "3. **Feedforward Layer**:\n",
        "   \\[\n",
        "   $\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$\n",
        "   \\]\n",
        "4. **Add & Norm**\n",
        "\n",
        "---\n",
        "\n",
        "##  Decoder Layer\n",
        "\n",
        "Mirip encoder, tetapi ditambahkan:\n",
        "- **Masked Self-Attention**: mencegah melihat token berikutnya\n",
        "- **Encoder‚ÄìDecoder Attention**: memperhatikan output encoder\n",
        "\n",
        "---\n",
        "\n",
        "##  Pretraining dan Fine-Tuning\n",
        "\n",
        "### üîπ Pretraining\n",
        "\n",
        "Model dilatih pada tugas besar berbasis data umum seperti:\n",
        "- Language Modeling (GPT)\n",
        "- Masked LM (BERT)\n",
        "- Translation (T5)\n",
        "\n",
        "Tujuan: membangun **representasi bahasa umum**\n",
        "\n",
        "### üîπ Fine-Tuning\n",
        "\n",
        "Model pretrained diadaptasi ke tugas spesifik:\n",
        "- Klasifikasi\n",
        "- NER\n",
        "- Question Answering\n",
        "\n",
        "Hanya perlu sedikit data ‚Üí efisiensi tinggi\n",
        "\n",
        "---\n",
        "\n",
        "##  BERT: Bidirectional Encoder Representations from Transformers\n",
        "\n",
        "- Hanya encoder stack\n",
        "- Dilatih dengan **Masked Language Modeling** (MLM):\n",
        "  - Acak token ‚Üí ganti dengan `[MASK]`, prediksi token asli\n",
        "- Dan **Next Sentence Prediction (NSP)**\n",
        "\n",
        "---\n",
        "\n",
        "##  GPT: Generative Pretrained Transformer\n",
        "\n",
        "- Hanya decoder stack\n",
        "- Dilatih dengan **causal language modeling**:\n",
        "  - Prediksi token berikutnya\n",
        "\n",
        "GPT cocok untuk:\n",
        "- Generatif: teks, kode, dialog\n",
        "\n",
        "---\n",
        "\n",
        "##  T5: Text-To-Text Transfer Transformer\n",
        "\n",
        "- Unified framework: semua tugas ‚Üí input & output teks\n",
        "- Contoh:\n",
        "  - Input: `translate English to German: That is good.`\n",
        "  - Output: `Das ist gut.`\n",
        "\n",
        "---\n",
        "\n",
        "##  Masking dan Causal Attention\n",
        "\n",
        "- **Padding Mask**: menghindari kontribusi token pad\n",
        "- **Look-ahead Mask**: mencegah prediksi token masa depan saat training autoregressive\n",
        "\n",
        "---\n",
        "\n",
        "##  Transfer Learning dalam NLP\n",
        "\n",
        "Pretraining besar (miliaran token) + Fine-tuning kecil (ribuan token):\n",
        "- Akurasi tinggi\n",
        "- Biaya rendah\n",
        "- Mengurangi kebutuhan anotasi manual\n",
        "\n",
        "---\n",
        "\n",
        "##  Evaluasi dan Efisiensi\n",
        "\n",
        "- Akurasi tugas NLP meningkat drastis (SQuAD, GLUE)\n",
        "- Teknik optimisasi:\n",
        "  - Mixed precision\n",
        "  - Knowledge distillation\n",
        "  - Parameter sharing (ALBERT)\n",
        "\n",
        "---\n",
        "\n",
        "##  Kesimpulan\n",
        "\n",
        "- Transformer menggantikan RNN untuk hampir semua tugas NLP\n",
        "- Self-attention memungkinkan pemrosesan paralel dan efisien\n",
        "- Pretrained transformer (BERT, GPT, T5) mendominasi NLP modern\n",
        "- Fine-tuning meningkatkan hasil bahkan dengan data kecil\n",
        "\n",
        "---\n",
        "\n",
        "##  Referensi\n",
        "\n",
        "G√©ron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*. O'Reilly Media."
      ],
      "metadata": {
        "id": "fU-ysWM3U8ot"
      }
    }
  ]
}