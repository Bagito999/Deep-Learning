{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqLBvXgom5rEMxxFAzpGBO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bagito999/Deep-Learning/blob/main/Chapter_18_Teori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Chapter 18 - Reinforcement Learning (RL)\n",
        "\n",
        "##  Tujuan Bab\n",
        "\n",
        "Memahami dasar-dasar Reinforcement Learning (RL), yaitu pendekatan Machine Learning di mana **agen belajar dari interaksi dengan lingkungan**.\n",
        "\n",
        "RL banyak digunakan dalam:\n",
        "- Game (AlphaGo, Dota2 AI)\n",
        "- Robotika\n",
        "- Sistem rekomendasi adaptif\n",
        "- Navigasi otonom\n",
        "\n",
        "---\n",
        "\n",
        "##  Perbedaan RL vs Supervised Learning\n",
        "\n",
        "| Supervised Learning          | Reinforcement Learning                   |\n",
        "|-----------------------------|------------------------------------------|\n",
        "| Diberi label                | Tidak diberi label, hanya reward         |\n",
        "| Satu langkah prediksi       | Multi-step interaksi                    |\n",
        "| Tidak interaktif            | Interaktif: tindakan memengaruhi hasil  |\n",
        "\n",
        "---\n",
        "\n",
        "##  Komponen Utama RL\n",
        "\n",
        "1. **Agent**: sistem pembelajar\n",
        "2. **Environment**: tempat agent beroperasi\n",
        "3. **State (s)**: representasi lingkungan saat ini\n",
        "4. **Action (a)**: tindakan yang diambil agent\n",
        "5. **Reward (r)**: umpan balik dari environment\n",
        "6. **Policy (π)**: strategi agent (mapping dari state ke action)\n",
        "7. **Value Function (V(s))**: ekspektasi reward dari suatu state\n",
        "8. **Q-Function (Q(s, a))**: ekspektasi reward dari state-action pair\n",
        "\n",
        "---\n",
        "\n",
        "##  Proses RL - Markov Decision Process (MDP)\n",
        "\n",
        "MDP mendefinisikan lingkungan RL sebagai:\n",
        "\n",
        "\\[\n",
        "$(S, A, P, R, \\gamma)$\n",
        "\\]\n",
        "\n",
        "- \\($ S $\\): ruang state\n",
        "- \\($ A $\\): ruang aksi\n",
        "- \\($ P(s'|s,a) $\\): probabilitas transisi\n",
        "- \\($ R(s,a) $\\): reward\n",
        "- \\($ \\gamma $\\): discount factor (0 < γ ≤ 1)\n",
        "\n",
        "---\n",
        "\n",
        "##  Objective\n",
        "\n",
        "Tujuan: maksimalkan **expected cumulative reward**:\n",
        "\n",
        "\\[\n",
        "$G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}$\n",
        "\\]\n",
        "\n",
        "Agent belajar policy \\($ \\pi(a|s) $\\) yang memaksimalkan nilai total ini.\n",
        "\n",
        "---\n",
        "\n",
        "##  Value Function\n",
        "\n",
        "### State Value:\n",
        "\n",
        "\\[\n",
        "$V^\\pi(s) = \\mathbb{E}_\\pi [G_t | S_t = s]$\n",
        "\\]\n",
        "\n",
        "### Action Value (Q-value):\n",
        "\n",
        "\\[\n",
        "$Q^\\pi(s, a) = \\mathbb{E}_\\pi [G_t | S_t = s, A_t = a]$\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "## Bellman Equation\n",
        "\n",
        "### Untuk \\($ V^\\pi(s) $\\):\n",
        "\n",
        "\\[\n",
        "$V^\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^\\pi(s')]$\n",
        "\\]\n",
        "\n",
        "### Untuk \\($ Q^\\pi(s,a) $\\):\n",
        "\n",
        "\\[\n",
        "$Q^\\pi(s,a) = \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s',a')]$\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "##  Policy vs Value-Based Methods\n",
        "\n",
        "- **Policy-based**: langsung belajar policy \\($ \\pi $\\) (misalnya: REINFORCE, PPO)\n",
        "- **Value-based**: belajar fungsi nilai, lalu derive policy (misalnya: Q-Learning, DQN)\n",
        "\n",
        "---\n",
        "\n",
        "##  Q-Learning\n",
        "\n",
        "Off-policy algorithm yang memperbarui Q-value:\n",
        "\n",
        "\\[\n",
        "$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s,a)]$\n",
        "\\]\n",
        "\n",
        "- \\($ \\alpha $\\): learning rate\n",
        "\n",
        "---\n",
        "\n",
        "##  Deep Q-Network (DQN)\n",
        "\n",
        "Gunakan Neural Network untuk memapprox Q-function:\n",
        "\n",
        "\\[\n",
        "$Q(s, a; \\theta) \\approx Q^\\ast(s, a)$\n",
        "\\]\n",
        "\n",
        "Fitur penting:\n",
        "- **Experience Replay**: menyimpan pengalaman di buffer\n",
        "- **Target Network**: stabilkan pembaruan Q dengan parameter tetap\n",
        "\n",
        "---\n",
        "\n",
        "##  Policy Gradient\n",
        "\n",
        "Langsung optimalkan policy parameter \\($ \\theta $\\):\n",
        "\n",
        "\\[\n",
        "$\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi [\\nabla_\\theta \\log \\pi_\\theta(a|s) Q^\\pi(s,a)]$\n",
        "\\]\n",
        "\n",
        "Contoh: **REINFORCE** algorithm\n",
        "\n",
        "---\n",
        "\n",
        "##  Actor–Critic\n",
        "\n",
        "Gabungkan:\n",
        "- **Actor**: mengupdate policy \\($ \\pi $\\)\n",
        "- **Critic**: mengevaluasi policy dengan fungsi nilai\n",
        "\n",
        "Lebih stabil dibanding REINFORCE murni\n",
        "\n",
        "---\n",
        "\n",
        "##  Advantage Function\n",
        "\n",
        "Mengukur seberapa bagus suatu aksi dibandingkan rata-rata:\n",
        "\n",
        "\\[\n",
        "$A(s,a) = Q(s,a) - V(s)$\n",
        "\\]\n",
        "\n",
        "Digunakan dalam algoritma seperti A2C, PPO\n",
        "\n",
        "---\n",
        "\n",
        "##  Algoritma Modern\n",
        "\n",
        "- **DQN**: Value-based, Deep Learning\n",
        "- **Double DQN**: kurangi overestimation Q\n",
        "- **Dueling DQN**: pisahkan V(s) dan A(s,a)\n",
        "- **PPO (Proximal Policy Optimization)**: policy gradient stabil dan efisien\n",
        "- **A3C/A2C**: parallel actor–critic\n",
        "\n",
        "---\n",
        "\n",
        "##  Kesimpulan\n",
        "\n",
        "- RL adalah paradigma pembelajaran berbasis interaksi dan reward\n",
        "- Tujuan: belajar policy optimal yang memaksimalkan cumulative reward\n",
        "- Banyak pendekatan: Q-learning, Policy Gradient, Actor–Critic\n",
        "- Deep RL memungkinkan pemecahan masalah kompleks (game, robotik)\n",
        "\n",
        "---\n",
        "\n",
        "##  Referensi\n",
        "\n",
        "- Géron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*\n"
      ],
      "metadata": {
        "id": "jn2lzxiuWbL1"
      }
    }
  ]
}
