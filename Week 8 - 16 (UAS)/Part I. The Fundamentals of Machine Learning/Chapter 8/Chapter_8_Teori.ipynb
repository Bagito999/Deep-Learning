{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhWYkqwzXLTPxfwHY5UHGt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bagito999/Deep-Learning/blob/main/Chapter_8_Teori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Chapter 8 - Dimensionality Reduction\n",
        "\n",
        "##  Mengapa Perlu Mengurangi Dimensi?\n",
        "\n",
        "Banyak masalah Machine Learning melibatkan ratusan hingga jutaan fitur. Dampaknya:\n",
        "- Melambatkan proses training\n",
        "- Meningkatkan risiko overfitting\n",
        "- Menurunkan akurasi generalisasi\n",
        "\n",
        "Motivasi utama:\n",
        "-  Mempercepat training\n",
        "-  Mengurangi noise\n",
        "-  Memungkinkan visualisasi (2D/3D)\n",
        "-  Kompresi data\n",
        "\n",
        "---\n",
        "\n",
        "##  The Curse of Dimensionality\n",
        "\n",
        "Fenomena tak intuitif di ruang berdimensi tinggi:\n",
        "- Titik data menjadi **jarang tersebar**\n",
        "- Rata-rata jarak antar titik meningkat drastis\n",
        "- Dimensi tinggi → prediksi makin tidak dapat diandalkan\n",
        "\n",
        "Contoh:\n",
        "- Di ruang 1 juta dimensi, rata-rata jarak antar titik bisa mencapai ~408.25\n",
        "- Dibutuhkan jumlah data eksponensial untuk mempertahankan densitas yang cukup tinggi\n",
        "\n",
        "---\n",
        "\n",
        "##  Dua Pendekatan Utama\n",
        "\n",
        "### 🔹 Projection\n",
        "\n",
        "Data sering kali berada pada subspace berdimensi lebih rendah. Kita bisa:\n",
        "- Menemukan subspace ini\n",
        "- Memproyeksikan data secara ortogonal ke sana\n",
        "\n",
        "Namun, tidak efektif untuk manifold nonlinier (contohnya Swiss Roll).\n",
        "\n",
        "### 🔸 Manifold Learning\n",
        "\n",
        "Asumsi manifold: data real-world cenderung berada pada manifold berdimensi rendah di dalam ruang berdimensi tinggi.\n",
        "\n",
        "Contoh:\n",
        "- MNIST → semua angka tulisan tangan terbentuk oleh garis yang terhubung\n",
        "- Swiss Roll → secara lokal 2D, tapi dilipat dalam 3D\n",
        "\n",
        "---\n",
        "\n",
        "##  PCA - Principal Component Analysis\n",
        "\n",
        "###  Tujuan\n",
        "Menemukan subspace yang mempertahankan variansi data sebanyak mungkin.\n",
        "\n",
        "###  Rumus\n",
        "\n",
        "#### Langkah 1: SVD\n",
        "\n",
        "\\[\n",
        "$X = U \\Sigma V^T$\n",
        "\\]\n",
        "\n",
        "- \\($ V $\\): matriks komponen utama (PC)\n",
        "- \\($ X $\\): data yang sudah di-center-kan\n",
        "\n",
        "#### Langkah 2: Proyeksi\n",
        "\n",
        "\\[\n",
        "$X_{proj} = X \\cdot W_d$\n",
        "\\]\n",
        "\n",
        "- \\($ W_d $\\): d kolom pertama dari \\($ V $\\)\n",
        "\n",
        "###  Explained Variance Ratio\n",
        "\n",
        "\\[\n",
        "$\\text{EVR}_k = \\frac{\\text{Var}(PC_k)}{\\text{Total Variance}}$\n",
        "\\]\n",
        "\n",
        "Pilih jumlah dimensi yang mempertahankan rasio varian tertentu (misalnya 95%).\n",
        "\n",
        "---\n",
        "\n",
        "##  PCA untuk Kompresi\n",
        "\n",
        "- Kurangi dimensi → simpan data lebih efisien\n",
        "- Kompres MNIST: 784 ➝ 154 fitur (95% varian dipertahankan)\n",
        "- Bisa decompress dengan:\n",
        "\n",
        "\\[\n",
        "$X_{recovered} = X_{proj} \\cdot W_d^T$\n",
        "\\]\n",
        "\n",
        "→ Disebut sebagai *reconstruction pre-image*.\n",
        "\n",
        "---\n",
        "\n",
        "##  Incremental PCA\n",
        "\n",
        "Digunakan saat dataset terlalu besar untuk dimuat ke memori:\n",
        "- Proses secara batch\n",
        "- Cocok untuk online learning\n",
        "\n",
        "---\n",
        "\n",
        "##  Kernel PCA\n",
        "\n",
        "Melakukan PCA di ruang berdimensi tinggi menggunakan kernel trick.\n",
        "\n",
        "### 🔸 Kernel populer:\n",
        "- Linear\n",
        "- RBF (Gaussian)\n",
        "- Sigmoid\n",
        "\n",
        "Tidak ada cara pasti memilih kernel terbaik → gunakan Grid Search berdasarkan performa tugas supervised (misalnya klasifikasi).\n",
        "\n",
        "---\n",
        "\n",
        "##  LLE - Locally Linear Embedding\n",
        "\n",
        "### Langkah 1: Rekonstruksi Lokal\n",
        "\n",
        "\\[\n",
        "$W = \\arg \\min_W \\sum_{i=1}^m \\left\\| x_i - \\sum_j w_{ij} x_j \\right\\|^2$\n",
        "\\]\n",
        "\n",
        "Syarat:\n",
        "- \\($ w_{ij} = 0 $\\) jika \\($ x_j $\\) bukan tetangga terdekat dari \\($ x_i $\\)\n",
        "- \\($ \\sum_j w_{ij} = 1 $\\)\n",
        "\n",
        "### Langkah 2: Mapping ke Ruang Rendah\n",
        "\n",
        "\\[\n",
        "$Z = \\arg \\min_Z \\sum_{i=1}^m \\left\\| z_i - \\sum_j w_{ij} z_j \\right\\|^2$\n",
        "\\]\n",
        "\n",
        "- Preservasi hubungan lokal\n",
        "- Cocok untuk data manifold yang dilipat\n",
        "\n",
        "---\n",
        "\n",
        "##  Teknik Lain\n",
        "\n",
        "### 🔹 Random Projections\n",
        "- Proyeksi acak ke dimensi lebih rendah\n",
        "- Preservasi jarak dijamin oleh *Johnson-Lindenstrauss Lemma*\n",
        "\n",
        "### 🔹 Multidimensional Scaling (MDS)\n",
        "- Mempertahankan jarak antar instance\n",
        "\n",
        "### 🔹 Isomap\n",
        "- Perluas MDS dengan geodesic distance\n",
        "\n",
        "### 🔹 t-SNE\n",
        "- Preservasi local structure (cluster)\n",
        "- Ideal untuk visualisasi\n",
        "\n",
        "### 🔹 LDA (Linear Discriminant Analysis)\n",
        "- Klasifikasi + proyeksi ke dimensi yang memisahkan kelas sebaik mungkin\n",
        "\n",
        "---\n",
        "\n",
        "##  Kesimpulan\n",
        "\n",
        "- Dimensionality Reduction mempercepat training dan memungkinkan visualisasi\n",
        "- PCA sangat efektif untuk kompresi dan reduksi linier\n",
        "- Kernel PCA dan LLE cocok untuk struktur non-linier\n",
        "- Pilih teknik berdasarkan dataset dan tujuan (kompresi, visualisasi, preprocessing)\n",
        "\n",
        "---\n",
        "\n",
        "##  Referensi\n",
        "\n",
        "Géron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*. O'Reilly Media.\n"
      ],
      "metadata": {
        "id": "PKZfVZYtkgu9"
      }
    }
  ]
}
