{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMz+Xtp8UiShB6zeXCtpxxo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bagito999/Deep-Learning/blob/main/Chapter_11_Teori.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Chapter 11 - Training Deep Neural Networks\n",
        "\n",
        "##  Tujuan Bab\n",
        "\n",
        "Memahami berbagai teknik untuk:\n",
        "- Mempercepat pelatihan jaringan saraf dalam\n",
        "- Meningkatkan konvergensi\n",
        "- Mengatasi overfitting dan vanishing gradient\n",
        "- Memilih arsitektur dan hyperparameter yang efektif\n",
        "\n",
        "---\n",
        "\n",
        "##  Tantangan Pelatihan DNN\n",
        "\n",
        "1. **Vanishing/Exploding Gradients**\n",
        "2. **Waktu pelatihan lama**\n",
        "3. **Kesulitan memilih arsitektur**\n",
        "4. **Overfitting**\n",
        "\n",
        "---\n",
        "\n",
        "##  Vanishing dan Exploding Gradients\n",
        "\n",
        "###  Masalah:\n",
        "Saat gradien terlalu kecil (vanishing) atau terlalu besar (exploding), training menjadi tidak stabil.\n",
        "\n",
        "Biasanya terjadi pada jaringan yang:\n",
        "- Terlalu dalam\n",
        "- Menggunakan aktivasi sigmoid/tanh\n",
        "\n",
        "###  Solusi:\n",
        "\n",
        "1. **Aktivasi ReLU**\n",
        "2. **Inisialisasi bobot yang tepat**\n",
        "3. **Batch Normalization**\n",
        "4. **Arsitektur shortcut seperti ResNet**\n",
        "\n",
        "---\n",
        "\n",
        "##  Inisialisasi Bobot\n",
        "\n",
        "### ðŸ”¹ Xavier (Glorot) Initialization:\n",
        "\n",
        "\\[\n",
        "$\\text{Var}(w) = \\frac{1}{n_{\\text{inputs}}}$\n",
        "\\]\n",
        "\n",
        "Cocok untuk sigmoid atau tanh.\n",
        "\n",
        "### ðŸ”¹ He Initialization:\n",
        "\n",
        "\\[\n",
        "$\\text{Var}(w) = \\frac{2}{n_{\\text{inputs}}}$\n",
        "\\]\n",
        "\n",
        "Cocok untuk ReLU dan turunannya.\n",
        "\n",
        "---\n",
        "\n",
        "##  Batch Normalization\n",
        "\n",
        "Normalisasi output layer (aktivasi) agar:\n",
        "- Memiliki mean â‰ˆ 0 dan std â‰ˆ 1\n",
        "- Mempercepat konvergensi\n",
        "- Mengurangi sensitivitas terhadap inisialisasi\n",
        "\n",
        "### ðŸ”¸ Rumus:\n",
        "\n",
        "\\[\n",
        "$\\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$\n",
        "\\]\n",
        "\\[\n",
        "$y^{(i)} = \\gamma \\hat{x}^{(i)} + \\beta$\n",
        "\\]\n",
        "\n",
        "- \\($ \\mu_B $\\), \\($ \\sigma_B $\\): mean dan std mini-batch\n",
        "- \\($ \\gamma $\\), \\($ \\beta $\\): parameter learnable\n",
        "\n",
        "---\n",
        "\n",
        "##  Optimizers\n",
        "\n",
        "### ðŸ”¹ SGD (Stochastic Gradient Descent)\n",
        "\n",
        "\\[\n",
        "$\\theta := \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta)$\n",
        "\\]\n",
        "\n",
        "### ðŸ”¸ Momentum\n",
        "\n",
        "\\[\n",
        "$v := \\beta v - \\eta \\nabla_\\theta J(\\theta)\n",
        "\\quad\\text{dan}\\quad \\theta := \\theta + v$\n",
        "\\]\n",
        "\n",
        "### ðŸ”¹ Nesterov Accelerated Gradient\n",
        "\n",
        "\\[\n",
        "$\\theta_{\\text{lookahead}} = \\theta + \\beta v$\n",
        "\\]\n",
        "\n",
        "Gradien dihitung dari posisi *lookahead*.\n",
        "\n",
        "### ðŸ”¸ AdaGrad\n",
        "\n",
        "Menyesuaikan learning rate berdasarkan gradien sebelumnya:\n",
        "\n",
        "\\[\n",
        "$\\theta_j := \\theta_j - \\frac{\\eta}{\\sqrt{G_{jj}} + \\epsilon} \\cdot g_j$\n",
        "\\]\n",
        "\n",
        "### ðŸ”¹ RMSProp\n",
        "\n",
        "Mirip AdaGrad tapi menggunakan rata-rata eksponensial dari kuadrat gradien.\n",
        "\n",
        "\\[\n",
        "$E[g^2]_t = \\beta E[g^2]_{t-1} + (1 - \\beta) g_t^2$\n",
        "\\]\n",
        "\n",
        "### ðŸ”¸ Adam (Adaptive Moment Estimation)\n",
        "\n",
        "Menggabungkan Momentum dan RMSProp:\n",
        "\n",
        "\\[\n",
        "$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$\n",
        "\\]\n",
        "\\[\n",
        "$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2$\n",
        "\\]\n",
        "\\[\n",
        "$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$\n",
        "\\]\n",
        "\\[\n",
        "$\\theta := \\theta - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "##  Early Stopping\n",
        "\n",
        "Berhenti training saat validasi loss mulai meningkat:\n",
        "\n",
        "- Mencegah overfitting\n",
        "- Menghemat waktu\n",
        "\n",
        "---\n",
        "\n",
        "##  Hyperparameter Tuning\n",
        "\n",
        "###  Parameter penting:\n",
        "- Learning rate\n",
        "- Ukuran batch\n",
        "- Jumlah hidden layer & neuron\n",
        "- Fungsi aktivasi\n",
        "- Optimizer\n",
        "\n",
        "###  Pendekatan:\n",
        "- Grid Search\n",
        "- Random Search\n",
        "- Bayesian Optimization\n",
        "\n",
        "---\n",
        "\n",
        "##  Model Checkpointing\n",
        "\n",
        "Menyimpan model terbaik berdasarkan validasi loss:\n",
        "- Gunakan untuk melanjutkan training\n",
        "- Diperlukan untuk produksi\n",
        "\n",
        "---\n",
        "\n",
        "##  Pretraining\n",
        "\n",
        "### ðŸ”¸ Unsupervised Pretraining:\n",
        "- Gunakan Autoencoder atau Restricted Boltzmann Machine untuk inisialisasi\n",
        "\n",
        "### ðŸ”¸ Transfer Learning:\n",
        "- Gunakan model pretrained (misal: dari ImageNet)\n",
        "- Fine-tune pada task baru\n",
        "\n",
        "---\n",
        "\n",
        "##  Penambahan Noise dan Regularisasi\n",
        "\n",
        "### ðŸ”¹ Dropout\n",
        "\n",
        "Saat training, neuron dinonaktifkan secara acak dengan probabilitas \\( p \\).\n",
        "\n",
        "\\[\n",
        "$\\tilde{h}_i = h_i \\cdot r_i \\quad \\text{dengan } r_i \\sim \\text{Bernoulli}(p)$\n",
        "\\]\n",
        "\n",
        "Pada inference, aktivasi diskalakan dengan \\($ p $\\).\n",
        "\n",
        "---\n",
        "\n",
        "##  Kesimpulan\n",
        "\n",
        "- Training DNN menantang karena masalah seperti vanishing gradients dan overfitting\n",
        "- Teknik seperti ReLU, He initialization, Batch Norm, dan optimizers modern sangat membantu\n",
        "- Regularisasi seperti dropout dan early stopping meningkatkan generalisasi\n",
        "- Transfer learning dan checkpointing sangat berguna dalam praktik\n",
        "\n",
        "---\n",
        "\n",
        "##  Referensi\n",
        "\n",
        "GÃ©ron, A. (2019). *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*. O'Reilly Media.\n"
      ],
      "metadata": {
        "id": "S9Dx3MJ3GoG6"
      }
    }
  ]
}